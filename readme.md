# Vision and Radar fusion

This project implements an object recognition system, likely for autonomous driving scenarios, using PyTorch. It appears to fuse data from cameras and radar sensors for improved detection.
![Model Output Visualization](output.gif)
## Project Structure

The project consists of the following key Python scripts:

* **`constants.py`**: Defines constants used throughout the project, such as image dimensions, grid size, class labels, and radar parameters.
* **`dataset_generator.py`**: Contains code to process raw data (images, bounding boxes, radar signals) and generate a preprocessed dataset suitable for training. It handles image loading, resizing, bounding box conversion to YOLO format, and radar data normalization.
* **`dataset_loader.py`**: Provides a PyTorch Dataset class to load the preprocessed data generated by `dataset_generator.py` for training and evaluation.
* **`car_recognition.py`**: Defines the convolutional neural network (CNN) architecture based on VGG16 for image-based object recognition. It also includes the YOLO loss function implementation.
* **`fusion.py`**: Implements the sensor fusion logic. It includes a network to process radar features (`RadarFeatureNetwork`) and a `FusionNetwork` that combines features from the camera (using the CNN from `car_recognition.py`) and radar data.
* **`train_object_recogniton.py`**: Script to train the image-only object recognition model (`CarRecorgnitionNetwork`).
* **`main.py`**: The main training script that trains the `FusionNetwork`, utilizing both image and radar data. It includes data loading, model setup, optimizer/scheduler configuration, and the training/validation loop.
* **`model_viewer.py` / `obj_viewer.py`**: Utility scripts to load a trained model (either the fusion model or the object recognition model) and visualize its predictions on sample or selected images by drawing bounding boxes.

## Core Functionality

1.  **Data Preprocessing**: Reads image, bounding box (2D and 3D), and radar data, preprocesses it, and saves it in a format ready for training.
2.  **Object Recognition (Image-based)**: Uses a CNN to detect objects (vehicles, pedestrians) in camera images.
3.  **Radar Feature Extraction**: Processes radar point cloud data, potentially incorporating camera position, using an MLP and attention mechanism.
4.  **Sensor Fusion**: Combines extracted features from the camera CNN and the radar network to make final object detection predictions, including bounding boxes, class probabilities, and potentially distance estimates.
5.  **Training**: Trains the models using the YOLO loss function and standard optimization techniques (Adam/SGD, learning rate scheduling).
6.  **Visualization**: Allows viewing model predictions overlaid on images.

## How to Run (General Steps)

1.  **Generate Dataset**: Run `dataset_generator.py` to preprocess your raw data.
2.  **Train Object Recognition Model (Optional but likely needed)**: Run `train_object_recogniton.py` to train the image-only model. A pre-trained model seems to be loaded in `main.py` and `model_viewer.py`.
3.  **Train Fusion Model**: Run `main.py` to train the fusion network.
4.  **View Results**: Use `model_viewer.py` or `obj_viewer.py` to visualize the predictions of a trained model.

*(Note: Ensure you have the necessary dataset and dependencies, primarily PyTorch and OpenCV, installed.)*